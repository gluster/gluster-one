# Colonizer common verify ID file for HPE Apollo 4200 Gen9 deployment
#
- shell: /bin/bash -c 'echo "Checking system resources..." > {{ fifo }}'

- assert:
    that:
      - ansible_processor_vcpus >= 32
      - ansible_product_name == "ProLiant XL420 Gen9"
      - ansible_memtotal_mb >= 64000
      - ansible_architecture == 'x86_64'
      - ansible_system_vendor == 'HP'
      - ansible_distribution == 'RedHat'
      - ansible_distribution_major_version == '7'
    msg: "System {{ ansible_host }} does not conform to the required hardware / operatign system requirements!"

- shell: /bin/bash -c 'echo "Checking subscriptions and repositories..." > {{ fifo }}'

- name: get subscription status
  shell: subscription-manager status
  register: sm_status_cmd
  changed_when: false

- assert:
    that:
      - sm_status_cmd | success
      - "'Overall Status: Current' in sm_status_cmd.stdout"
    msg: "System {{ ansible_host }} is not properly subscribed to repos!"

- name: get enabled repositories
  shell: yum repolist enabled
  args:
    warn: no
  register: yum_repolist_cmd
  changed_when: false

- assert:
    that:
      - yum_repolist_cmd | success
      - "yum_repolist_cmd.stdout | search('(?<!-)rhel-7-server-rpms/x86_64')"
      - "yum_repolist_cmd.stdout | search('rh-gluster-3-for-rhel-7-server-rpms/x86_64')"
      - "yum_repolist_cmd.stdout | search('rh-gluster-3-nfs-for-rhel-7-server-rpms/x86_64')"
      - "yum_repolist_cmd.stdout | search('rh-gluster-3-samba-for-rhel-7-server-rpms/x86_64')"
      - "yum_repolist_cmd.stdout | search('rhel-ha-for-rhel-7-server-rpms/x86_64')"
    msg: "System {{ ansible_host }} does not have the required repositories enabled. Please enable rhel-7-server-rpms, rh-gluster-3-for-rhel-7-server-rpms, rh-gluster-3-nfs-for-rhel-7-server-rpms,  rh-gluster-3-samba-for-rhel-7-server-rpms and rhel-ha-for-rhel-7-server-rpms!"

- shell: /bin/bash -c 'echo "Checking RPM package requirements..." > {{ fifo }}'

  #NOTE: We shouldn't need zeroconf for this type of deployment without auto-discovery
- name: search for gluster and gluster-colonizer packages
  #shell: yum list all redhat-storage-server gluster-colonizer gluster-zeroconf-avahi
  shell: yum list all redhat-storage-server gluster-colonizer
  args:
    warn: no
  register: yum_list_gluster_pkgs_cmd
  changed_when: false

- assert:
    that:
      - yum_list_gluster_pkgs_cmd | success
      - "yum_list_gluster_pkgs_cmd.stdout | search('redhat-storage-server.noarch')"
      - "yum_list_gluster_pkgs_cmd.stdout | search('gluster-colonizer.noarch')"
     #- "yum_list_gluster_pkgs_cmd.stdout | search('gluster-zeroconf-avahi.noarch')"
     #msg: "System {{ ansible_host }} does not have the redhat-storage-server, gluster-colonizer and/or gluster-zeroconf-avahi packages available!"
    msg: "System {{ ansible_host }} does not have the redhat-storage-server or the gluster-colonizer packages available!"

- name: add HPE ServicePack for ProLiant, disable by default
  yum_repository:
    name: hpe-spp
    baseurl: https://downloads.linux.hpe.com/SDR/repo/spp/redhat/7Server/x86_64/current/
    description: HPE ServicePack for Proliant
    gpgcheck: no
    enabled: no
    state: present
  when: enable_hpe_spp == True

- name: search for hpssacli package
  shell: yum {{ (enable_hpe_spp == True) | ternary('--enablerepo=hpe-spp', '') }} list all hpssacli
  args:
    warn: no
  register: yum_list_available_hpssacli_cmd
  failed_when: false
  changed_when: false

- assert:
    that:
      - yum_list_available_hpssacli_cmd.rc == 0 or yum_list_available_hpssacli_cmd.rc == 1
      - "'hpssacli.x86_64' in yum_list_available_hpssacli_cmd.stdout"
    msg: "System {{ ansible_host }} cannot find hpssacli package!"

- name: install hpssacli from hpe-spp repo
  yum:
    name: hpssacli
    state: present
    enablerepo: hpe-spp
  when: enable_hpe_spp == True

- name: install hpssacli from system configured repositories
  yum:
    name: hpssacli
    state: present
  when: enable_hpe_spp != True

- name: enumerate raid controller using hpssacli
  shell: hpssacli controller all show
  register: hpssacli_controller_show_cmd
  changed_when: false

- shell: /bin/bash -c 'echo "Checking block devices with hpssacli..." > {{ fifo }}'

- assert:
    that:
      - "hpssacli_controller_show_cmd.stdout | search('Smart Array (.*) in Slot ([0-9])')"
    msg: "System {{ ansible_host}} does not have a SmartArray RAID controller in the system."

- assert:
    that:
      - "hpssacli_controller_show_cmd.stdout | regex_findall('Smart Array .* in Slot ([0-9])', '\\1') | length <= 1"
    msg: "System {{ ansible_host}} has more than one SmartArray RAID controller in the system. This is not supported."

- set_fact:
    array_controller_slot: "{{ hpssacli_controller_show_cmd.stdout | regex_findall('Smart Array .* in Slot ([0-9])', '\\1') | first }}"

- name: enumerate physical drives using hpssacli
  shell: hpssacli controller slot={{ array_controller_slot }} pd all show | grep physicaldrive
  changed_when: false
  failed_when: false
  register: hpssacli_pd_show_cmd

  #FIXME: Don't hard-code expected disk number here
- assert:
    that:
      - hpssacli_pd_show_cmd.stdout_lines | length == 26
    msg: "26 disks expected, but {{ hpssacli_pd_show_cmd.stdout_lines | length }} found on system {{ ansible_host }}!"

- assert:
    that:
      - "'Fail' not in hpssacli_pd_show_cmd.stdout"
    msg: "System {{ ansible_host }} reports at least one disk in a failing state!"

- name: confirm configuration of OS logical drive
  shell: hpssacli controller slot={{ array_controller_slot }} ld 1 show
  changed_when: false
  failed_when: false
  register: hpssacli_os_ld_show_cmd

- assert:
    that:
      - hpssacli_os_ld_show_cmd.stdout | regex_search('(Fault\ Tolerance\:\ 1)')
      - hpssacli_os_ld_show_cmd.stdout | regex_findall('(physicaldrive)') | length == 2
      - hpssacli_os_ld_show_cmd.stdout | regex_search('(Disk\ Name\:\ \/dev\/sda)')
    msg: "System {{ ansible_host }} does not have boot disks properly configured as the first device in RAID 1!"

#FIXME: This check is problematic since it doesn't allow for non-NVMe cache devices
#- shell: /bin/bash -c 'echo "Checking NVMe devices..." > {{ fifo }}'
#
#- name: enumerate physical NVMe devices
#  shell: lsblk --output NAME /dev/nvme*
#  changed_when: false
#  register: lsblk_nvme_cmd
#
#- assert:
#    that:
#      - "{{ lsblk_nvme_cmd.stdout.find('nvme0n1') }}"
#      - "{{ lsblk_nvme_cmd.stdout.find('nvme1n1') }}"
#    msg: "System {{ ansible_host }} does not have the required NVMe devices present. Please make sure they show up as /dev/nvme0n1 and /dev/nvme1n1 in the system."

- shell: /bin/bash -c 'echo "Checking for iptables (unsupported)..." > {{ fifo }}'

- name: search for iptables-services package
  shell: yum list installed iptables-services
  args:
    warn: no
  register: yum_list_installed_iptables_cmd
  changed_when: false
  failed_when: false

- assert:
    that:
      - "'iptables-services.x86_64' not in yum_list_installed_iptables_cmd.stdout"
    msg: "System {{ ansible_host }} has the iptables-services package installed. Please remove it in favor of firewalld!"

#FIXME: This is problematic because network interface are hard-coded instead
#       of using the interfaces defined in the OEMID file.
- shell: /bin/bash -c 'echo "Checking network interfaces..." > {{ fifo }}'

- assert:
    that:
      - ansible_eno1 is defined
      - ansible_ens1f0 is defined
      - ansible_ens2f0 is defined
    msg: "System {{ ansible_host }} does not have the expected NICs available, is using a different driver or has the NICs in the wrong PCIe slots. Please ensure that minimally eno1, ens1f0 and ens2f0 are available."

- assert:
    that:
      - ansible_eno1.active == true
      - ansible_eno1.device == ansible_default_ipv4.alias
    msg: "System {{ ansible_host }} does not use eno1 as it's management network."

- assert:
    that:
      - ansible_ens1f0.active == true
      - ansible_ens1f0.speed == 10000
      - ansible_ens2f0.active == true
      - ansible_ens2f0.speed == 10000
    msg: "System {{ ansible_host }}'s network adapters ens1f0 and/or ens2f0 have either not negotiated the correct connection speed or are disconnected"

- shell: /bin/bash -c 'echo "Checking selinux..." > {{ fifo }}'

# Depends on presence of package python-dnf
- name: Install dependency for ansible selinux management
  package:
    name: libselinux-python.x86_64
    state: present

- selinux:
    policy: targeted
    state: enforcing
